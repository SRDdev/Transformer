{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac355e68",
   "metadata": {},
   "source": [
    "# MultiHeaded Attention\n",
    "\n",
    "Multi Headed Attention is the parellalized version of self attention in which we calculate self attention for multiple vectors at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d3b1cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d809c410",
   "metadata": {},
   "source": [
    "### Multi Headed Params\n",
    "- **Sequence Length** : The actual input words which we have.\n",
    "- **Input Dimensions** : The dimensions of each input word-vec. Usually 512\n",
    "- **Model Dimensions** : Max number of input words the model will take.\n",
    "\n",
    "\n",
    "_Our case_ : \"My name is Shreyas\"\n",
    "- **Sequence Length** = 4   as we have only 4 words\n",
    "- **Input Dimensions** = 512 as we will encode each word into [1x512]\n",
    "- **Model Dimensions** = 512 as we will have maximum input length as 512\n",
    "\n",
    "We will create a tensor of this shape (batch_size, sequence_length, input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "494c53bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vector :torch.Size([1, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 4\n",
    "batch_size = 1\n",
    "input_dim = 512\n",
    "d_model = 512\n",
    "\n",
    "x = torch.randn((batch_size, sequence_length, input_dim))\n",
    "\n",
    "print(f\"Size of the vector :{x.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b97487",
   "metadata": {},
   "source": [
    "### Calculating QKV vectors\n",
    "\n",
    "We basically copy the initial vector 3 times and multiply them with different filters of the size [512x512]. These filters are learnable parameters and hence we use them in a linear layer.\n",
    "\n",
    "The output of this will generate a matrix QKV of dimensions [1, 4, 1536]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e664eea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv_layer = nn.Linear(input_dim , 3 * d_model)\n",
    "qkv = qkv_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052d0299",
   "metadata": {},
   "source": [
    "Optional (Visulalize the QKV distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f834e519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# y_val = torch.histc(qkv, bins=200, min=-3, max=3)\n",
    "# x_val = np.arange(-1, 1, 0.01) * 3\n",
    "# plt.bar(x_val, y_val, align='center')\n",
    "# plt.title('qkv distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15078e79",
   "metadata": {},
   "source": [
    "### Heads\n",
    "The original Transformer paper had 8 Heads which calculated the self attention, so we will stick with it.\n",
    "\n",
    "The dimension of each input going into the head also needs to be calculated and stored.\n",
    "`head_dim` = (max input length) / (number of heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d851aa18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "The combine shape of the matrix will be : torch.Size([1, 4, 8, 192])\n"
     ]
    }
   ],
   "source": [
    "num_heads = 8\n",
    "head_dim = d_model // num_heads\n",
    "qkv = qkv.reshape(batch_size, sequence_length, num_heads, 3 * head_dim)\n",
    "print(head_dim)\n",
    "print(f\"The combine shape of the matrix will be : {qkv.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8558d9f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 192])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv = qkv.permute(0, 2, 1, 3) # [batch_size, num_heads, sequence_length, 3*head_dim]\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c60c73e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 8, 64]),\n",
       " torch.Size([1, 4, 8, 64]),\n",
       " torch.Size([1, 4, 8, 64]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split them into Q,K,V. This is done for each word embedding\n",
    "q, k, v = qkv.chunk(3, dim=-1)\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b20110",
   "metadata": {},
   "source": [
    "##### The Self Attention Equation is given\n",
    "\n",
    "$$Self-Attention = softmax(\\frac{Q.K^T}{\\sqrt(d_k)}+M)V$$\n",
    "\n",
    "\n",
    "The `Q.K^T` operation can be thought of as a similarity measure between the query and key vectors. The dot product of two vectors measures the **cosine similarity** between them, which ranges from -1 (opposite directions) to 1 (same direction). \n",
    "\n",
    "The $\\sqrt d_k$ is included in the equation to reduce the variance of the cosine similarity calculated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "798d95f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 8, 64])\n",
      "torch.Size([1, 4, 64, 8])\n"
     ]
    }
   ],
   "source": [
    "d_k = q.size()[-1]\n",
    "print(k.shape)\n",
    "print(k.transpose(-2, -1).shape)\n",
    "# scaled = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "# scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df28cc7e",
   "metadata": {},
   "source": [
    "### Masking\n",
    "\n",
    "This is generally required in the decoder part of the Transformer Architecture. This is to ensure words dont get context from words generated in the future. Not used in encoders.\n",
    "\n",
    "The generated mask is a upper trangular mask. Continuing the last example if the 1st row is a vector for My , \n",
    "them My is only allowed to look at My and no other word.\n",
    "Similarly as you will notice , Name which is represented by the second row is only alloed to view My & Name \n",
    "for predicting the next word in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817460fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.full(scaled.size() , float('-inf'))\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "mask[0][1] # mask for input to a single head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03454188",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# the entire input tensor after masking \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Use ```(scaled + mask)[0][0]``` to check the input tensor for 1st word in the input\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m (\u001b[43mscaled\u001b[49m \u001b[38;5;241m+\u001b[39m mask)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scaled' is not defined"
     ]
    }
   ],
   "source": [
    "# the entire input tensor after masking \n",
    "# Use ```(scaled + mask)[0][0]``` to check the input tensor for 1st word in the input\n",
    "(scaled + mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d58b20e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled += mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454818c9",
   "metadata": {},
   "source": [
    "### SoftMax\n",
    "\n",
    "It is used to convert a output vector into a probability distribution. This allows the sum of all the values in the output to sum up to 1. Allowing no value in the output to go beyond the probability of 100%.\n",
    "\n",
    "$$softmax = \\frac{e^{x_i}}{\\sum_je^x_j}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3ad5050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = F.softmax(scaled, dim=-1)\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "533cf5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3573, 0.6427, 0.0000, 0.0000],\n",
       "        [0.3879, 0.3296, 0.2825, 0.0000],\n",
       "        [0.2939, 0.3040, 0.2177, 0.1844]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the 1st word of input tensor after masked attention\n",
    "# Use ```attention``` to check the entire input tensors attention matrix\n",
    "attention[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef223982",
   "metadata": {},
   "source": [
    "Now by multiplying the attention matrix with the Value matrix, we generate a new matrix which tells the model which vectors are important to predict which output vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12236c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 64])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#outputs \n",
    "values = torch.matmul(attention, v)\n",
    "values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea2cca8",
   "metadata": {},
   "source": [
    "# MultiHeaded Attention Class\n",
    "\n",
    "Now lets combine all the things we did into a single class of python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0717cd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled += mask\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = nn.Linear(input_dim , 3 * d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, sequence_length, input_dim = x.size()\n",
    "        print(f\"x.size(): {x.size()}\")\n",
    "        qkv = self.qkv_layer(x)\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        print(f\"q size: {q.size()}, k size: {k.size()}, v size: {v.size()}, \")\n",
    "        values, attention = scaled_dot_product(q, k, v, mask)\n",
    "        print(f\"values.size(): {values.size()}, attention.size:{ attention.size()} \")\n",
    "        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
    "        print(f\"values.size(): {values.size()}\")\n",
    "        out = self.linear_layer(values)\n",
    "        print(f\"out.size(): {out.size()}\")\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "797a97eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.size(): torch.Size([30, 5, 1024])\n",
      "qkv.size(): torch.Size([30, 5, 1536])\n",
      "qkv.size(): torch.Size([30, 5, 8, 192])\n",
      "qkv.size(): torch.Size([30, 8, 5, 192])\n",
      "q size: torch.Size([30, 8, 5, 64]), k size: torch.Size([30, 8, 5, 64]), v size: torch.Size([30, 8, 5, 64]), \n",
      "values.size(): torch.Size([30, 8, 5, 64]), attention.size:torch.Size([30, 8, 5, 5]) \n",
      "values.size(): torch.Size([30, 5, 512])\n",
      "out.size(): torch.Size([30, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "input_dim = 1024\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "\n",
    "batch_size = 30\n",
    "sequence_length = 5\n",
    "x = torch.randn( (batch_size, sequence_length, input_dim) )\n",
    "\n",
    "model = MultiheadAttention(input_dim, d_model, num_heads)\n",
    "out = model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff79006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
