
<img src="https://github.com/SRDdev/Transformer/blob/3d7eb04bd81a283360724effab6b3cc3bdde93a6/Banner"/>

# Transformers

Welcome to Transformers a repository for understanding how the state-of-the-art Transformers work. In this repository you will find how to build a Transformer from scratch and also how to use the SOTA Transformers from ðŸ¤—Huggingface.

```
PS: This was a weekend project I did a year ago and thought of publishing now !
```

## Introduction to Transformers
The Transformers architecture is a type of deep neural network architecture that has revolutionized the field of natural language processing (NLP) and has been extended to many other applications such as computer vision, speech recognition, and recommendation systems. It was first introduced by Vaswani et al. in their seminal paper "Attention Is All You Need" in 2017, and has since become the state-of-the-art architecture for various NLP tasks. In this chapter, we will provide a detailed overview of the Transformers architecture, its components, and its working mechanism.


## ðŸ“‚Contents
Building a Transformer from scratch.
| Topic | Link |
|-------|------|
| Transformers Overview | [Link](https://github.com/SRDdev/Transformers/blob/a64264dfbaf077467d3a25e14298931f4b2ecc9d/Transformer_Overview.pdf) |
| Sentence Tokenizer | [Code](https://github.com/SRDdev/Transformers/blob/9ff7f6ddf27189ff290cc9560b024d627c3cc1a8/Building%20Transformer/Sentence%20Tokenization.ipynb) |
| Positional Encodings | [Code](https://github.com/SRDdev/Transformers/blob/a64264dfbaf077467d3a25e14298931f4b2ecc9d/Building%20Transformer/Positional%20Encoding.ipynb) |
| Layer Normalization | [Code](https://github.com/SRDdev/Transformers/blob/a64264dfbaf077467d3a25e14298931f4b2ecc9d/Building%20Transformer/Layer%20Normalization.ipynb) |
| Self Attention | [Code](https://github.com/SRDdev/Transformers/blob/a64264dfbaf077467d3a25e14298931f4b2ecc9d/Building%20Transformer/Self%20Attention.ipynb) |
| MultiHeaded Attention | [Code](https://github.com/SRDdev/Transformers/blob/a64264dfbaf077467d3a25e14298931f4b2ecc9d/Building%20Transformer/MultiHeaded%20Attention.ipynb) |
| Encoder | [Code](https://github.com/SRDdev/Transformers/blob/a64264dfbaf077467d3a25e14298931f4b2ecc9d/Building%20Transformer/Encoder.ipynb) |
| Decoder | [Code](https://github.com/SRDdev/Transformers/blob/a64264dfbaf077467d3a25e14298931f4b2ecc9d/Building%20Transformer/Decoder.ipynb) |
| Transformer | [Code](https://github.com/SRDdev/Transformers/blob/a64264dfbaf077467d3a25e14298931f4b2ecc9d/Building%20Transformer/Transformer.py) |



## ðŸ¤— HuggingFace Transformers

1. [Installation](https://github.com/SRDdev/Transformers/blob/master/HuggingFace%20Transformers/Installation.md) 
2. [Pipeline](https://github.com/SRDdev/Transformers/blob/master/HuggingFace%20Transformers/Pipeline.md)



## ðŸ«‚ Contributions
Contributions to this repo are welcome and encouraged! If you would like to contribute, please follow these guidelines in `contribution.md`

If you have any questions about how to contribute, please open an issue in the repo or reach out to the project maintainers.

Thank you for your contributions!


## ðŸ“ƒPapers
1. [Attention is all you need](https://arxiv.org/abs/1706.03762)

## Appreciation
This repository is possible only because of [Ajay Halthor](https://github.com/ajhalthor) & the entire repository credits go to him. This repository is just my personal take on transformers after learning it from him.

## Citations
```
@citation{ 
  SRDdev/Transformer,
  author = {Shreyas Dixit},
  year = {2023},
  url = {https://huggingface.co/SRDdev/Transformer}
}
```
